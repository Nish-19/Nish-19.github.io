---
title: 'Mathematics for ML and DL'
date: 2020-05-26
permalink: /posts/2020/05/blog-post-1/
tags:
  - Machine Learning
  - Mathematics
---

![image](https://user-images.githubusercontent.com/41947720/82878340-000d5180-9f59-11ea-8293-8d653df784e7.png)

Ever wondered what goes on in neural networks? Ever tried to calculate the gradients of the loss function after each pass? If yes, then you must have realized the role played by mathematical concepts here. If no, then you are at the right place.

In this blog post we will see the topics of Mathematics that are the most crucial for understanding and appreciating machine learning algorithms. Machine Learning has become very popular nowadays with developers and undergraduate college students, given the plethora of courses out there and the ease of building and running neural networks (thanks to google colab!). But, how many of these "machine-learners" actually understand the maths behind what is going on? Is it important? Well, not so much if you just want to play around with it, but an emphatic yes if you want to pursue research projects or come up with your new ideas.

Having said this, let's dive into our topics.

## Linear Algebra

![image](https://user-images.githubusercontent.com/41947720/82879763-0bfa1300-9f5b-11ea-9657-fa40c4225fcc.png)

Linear algebra is the study of vectors (n-dimensional in general) and the operations associated with them. In the context of Machine Learning, we use vectors, matrices and tensors to concisely represent data. By doing so, we can eliminate confusions and write the equations occurring in the ML models in a succinct and short-hand form.

![image](https://user-images.githubusercontent.com/41947720/82880622-516b1000-9f5c-11ea-9d17-fd289a03d65c.png)

The above image represents the computations taking place in a single neuron. We are calculating the weighted sum of the inputs and passing it through a non-linearity (function f() here).

![image](https://user-images.githubusercontent.com/41947720/82881001-bf173c00-9f5c-11ea-9d51-c28f6098c7ee.png)

We can observe the compact mathematical representation of the  equation in the above image. Here b + x.w is obtained by matrix multiplications and addition which is a topic under Linear algebra.   

Now, let's look at a bigger picture of the entire neural network from LA's perspective (LA - Linear Algebra and not Los Angeles ðŸ˜‰).

Consider the below NN architecture with 3 units in the input layer, 4 each in 2 hidden layers and 1 unit in the final layer (looks familiar ðŸ¤”- hmmm maybe of a binary classification task)

![image](https://user-images.githubusercontent.com/41947720/82881643-93e11c80-9f5d-11ea-9f0e-abe0d039ab66.png)

Can we represent the final output in a single line as a function of the input? (Hint - The diagram has weight matrices). The answer is yes. At each layer if we apply the formula shown above and combine all the three then we are done. Seems easy, isn't it? But how is this going to help? Well, this representation comes handy when we are calculating gradients of the loss function with respect to the network parameters during backpropagation, which we will look at next.

![image](https://user-images.githubusercontent.com/41947720/82882732-1b7b5b00-9f5f-11ea-9873-a1df4a649dce.png)

Also, the concepts of basis, eigen-values, eigen-vectors, singular-value-decomposition are very crucial (Ex: they form the pillars of PCA (principal component analysis) which is a very important concept in Machine Learning (we won't be discussing it here)). Having seen this let's move on to the next topic i.e. Multivariate Calculus.

## Multivariate Calculus

Wikipedia defines Calculus as the mathematical study of continuous change and so it is. Most of us are familiar with the concepts of integration and differentiation in calculus but what role do they have to play here? As said, we can view ML models as a computational box where the inputs is undergoing changes in some stages and finally changing to the output (a complicated way of saying output is some function of the input ðŸ˜‰). Now, in the training process our goal is to reduce the loss (or to optimize the model on the training data). This is where the theory of continuous change comes.

For reducing the loss function we have to tweak the weights and biases (collectively the parameters of the model), more specifically we have to move "opposite to the direction of the gradient".

![image](https://user-images.githubusercontent.com/41947720/82884842-f20ffe80-9f61-11ea-85b5-59f4fcabe909.png)

Consider this graph (here error surface), let us assume the z-axis represents the error and the x and y axes represent the parameters of the model. Now for reducing the error (or loss), we have to move in the x-y plane in the direction opposite to the gradient of the loss function wrt the parameters x and y (note - I'm not discussing the proof here). This commonly involves *vectorial differentiation* and the *chain-rule* over the loss function to some kth layer. Thankfully, we don't have to perform it ourselves ( we have Tensorflow and pytorch!). However, it is important to know about it.

The image below shows the chain rule of differentiation.

![image](https://user-images.githubusercontent.com/41947720/82885427-cd685680-9f62-11ea-9dc6-eb388720d19e.png)

Having a general idea of Calculus helps while carrying out research and building new models from scratch. Having said this, let's move on to the final topic Probability and Statistics.

## Probability and Statistics   

*the bedrock of ML*   

![image](https://user-images.githubusercontent.com/41947720/82890331-9f3a4500-9f69-11ea-84ed-5d7ce9eab0d3.png)   


Probability is the study of uncertainty. It is a science that quantifies the likely-hood of the occurrence of the events of interest in a given setting. But, how is it related to ML? Well, isn't ML about developing predictive models from uncertain data? Here is where Probability comes in. Probability is used almost everywhere in ML from defining [cross-entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) as the loss function of a classification task, [sampling data from specific gaussian distributions](https://en.wikipedia.org/wiki/Normal_distribution), usage in algorithms like [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) to study of [Bayesian Machine Learning](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/) (a separate branch of ML based on Bayesian inferences). Having a clear understanding of Probability is a *must* if you want to excel in ML.

![image](https://user-images.githubusercontent.com/41947720/82890578-09eb8080-9f6a-11ea-92db-fad856eed3eb.png)

*The formula above shows Kullback - Liebler Divergence to calculate the similarity between two probability distributions*

Why is Statistics used along with Probability? Statistics is the study of data (includes its collection, organization and analysis). Mean, median, quartiles, variance, and standard deviation which are statistical topics are dealt with in the form of evaluation of expectation values of probability distribution functions along with their other properties.

Since we deal with huge data in ML models, the task of organizing them and preparing them to be fed to the model is dealt by statistics (Ex: We enforce batch normalization before feeding image data to deep CNNs (convolutional neural networks))

![image](https://user-images.githubusercontent.com/41947720/82890725-40290000-9f6a-11ea-8176-33e522c3abe4.png)

*The above figure is of a common data distribution considered in ML (the gaussian distribution)*

Prob-Stat is indeed the most important topic of maths used in ML.

### Resources for Learning

* Linear Algebra -
  * [Khan Academy](https://www.khanacademy.org/math/linear-algebra)
  * [3 blue-1 brown](https://www.youtube.com/watch?v=kjBOesZCoqc&list=PL0-GT3co4r2y2YErbmuJw2L5tW4Ew2O5B)

* [Calculus](https://www.coursera.org/learn/multivariate-calculus-machine-learning)

* Probability and Statistics
  * [Khan Academy](https://www.khanacademy.org/math/statistics-probability)
  * [Machine Learning Mastery Blog](https://machinelearningmastery.com/probability-for-machine-learning-7-day-mini-course/)

#### Bonus

[For book lovers](https://mml-book.github.io/book/mml-book.pdf)     

[A complete course](https://www.coursera.org/specializations/mathematics-machine-learning)

## Conclusion

![image](https://user-images.githubusercontent.com/41947720/82891460-67340180-9f6b-11ea-82bb-9960d7538913.png)

This post helps the reader to understand why Mathematics is used in ML and how exactly. The explanations provided are a motivation for further learning. I have also provided Resources for each topic discussed.   

For more ML and DL content stay tuned!