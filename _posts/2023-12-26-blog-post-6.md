---
title: 'Stable Baselines 3 Tutorial (Computerized Adaptive Testing)'
date: 2023-12-26
permalink: /posts/2023/12/blog-post-6/
tags:
  - Reinforcement Learning
  - Stable Baselines 3
  - Computerized Adaptive Testing 
  - Educational Data Mining 
---

![pic](https://github.com/Nish-19/SB3-tutorial/assets/41947720/cc050964-5e21-43a2-8a14-a13654b68d6d)

<figcaption style="text-align: center;">Figure 1: Figure showing the MDP</figcaption>

The goal of this blog is to present a tutorial on [Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/), a popular Reinforcement Learning library with focus on implementing a custom environment and a custom policy. We will first describe our problem statement, discuss the MDP (Markov Decision Process), discuss the algorithms - [PPO](https://openai.com/research/openai-baselines-ppo), custom feature extractor PPO and custom policy (lstm bilinear policy with PPO). 

## Problem Statement 

In the realm of computer science education, evaluating coding homework poses unique challenges that traditional methods struggle to address. Auto-grading systems, which automatically assess correctness and functionality using test cases, offer a valuable solution. However, as the complexity of coding assignments grows, the sheer volume of test cases can hamper system efficiency. Inspired by Computerized Adaptive Testing (CAT), which optimizes assessments based on prior responses, we aim to develop a policy that selects a minimal yet effective set of test cases, ensuring swift and accurate evaluations for both students and instructors.

## Related Work (Item Response Theory)

In Computerized Adaptive Testing (CAT), there are four essential components:

1. **Knowledge Level Estimator:** Assesses the student's current knowledge based on their responses to previously selected items.
2. **Response Model:** Estimates the likelihood of a student answering a specific item correctly using the current knowledge level and item features.
3. **Pool of Available Items:** A collection of test items from which the adaptive system selects questions.
4. **Item Selection Algorithm:** Chooses the next most informative item based on the response model output.

The widely used Item-Response-Theory (IRT) model, specifically the simplest form (1PL), is often employed. In its basic form, the 1PL model is defined as:

$$ P(Y_{i, j} = 1) = \sigma(\theta_j - b_i) $$

Here, \( \theta_j \) represents the student's knowledge level, and \( b_i \) is the difficulty of item \( i \).


## MDP Formulation

### Notation
We denote the programming question statement as **q**; the solution (correct) code to the question as **a**; student *j*'s code as **c<sub>j</sub>**; student *j*'s ground truth code quality/ ability as **θ<sub>j</sub>**; student *j*'s current code quality estimate after selecting kth test cases as **θ̂<sub>j, k</sub>**; the total number of K test cases as **t<sub>1, 2,..., K</sub>** and each test case's feature as **d<sub>k</sub>**; and the ground truth value of whether the student *j*'s code passes test case *k* or not as **Y<sub>k, j</sub>**.

### MDP Setting
In our MDP setting, the environment contains two models: 1. Large Language Model (LLM<sub>e</sub>) 2.IRT-based model (IRT). The policy contains two models: 1. Large Language Model (LLM<sub>p</sub>) 2.Time-distributed long Short-Term Memory model (LSTM). During training, we use the same frozen LLM for LLM<sub>e</sub> and LLM<sub>p</sub> for simplicity. Figure 1 shows our MDP. 

### State Definition
We represent our state as the LLM embeddings of the question statement, the solution code, and the test cases recommended so far minus the embeddings of the question statement, the student code, and the test cases recommended so far, **S<sub>k</sub> = LLM<sub>e</sub>(q, a, t<sub>1..k</sub>) - LLM<sub>e</sub>(q, c<sub>j</sub>, t<sub>1..k</sub>)**.

### Action Definition
Initially, We obtain a matrix **Q** containing the embeddings of all the test cases **t<sub>1, 2,...,K</sub>** with LLM<sub>p</sub>, which is used for selecting the action. At every time step *k*, we first calculate the current hidden state **h<sub>k</sub>** using LSTM with the state **S<sub>k</sub>** as the input. Next, we project **h<sub>k</sub>** to the space of test cases **Q** using a bi-linear projection matrix **W** to obtain the logits **L'**. Finally, we apply the softmax function to the logits **L'** to obtain a distribution over the test cases and choose the test case with the highest probability, which is **A<sub>k</sub>**.

- **Q** = {LLM<sub>p</sub>(t<sub>k</sub>)}<sub>k ∈ {1..K}</sub>
- **h<sub>k</sub>** = LSTM(**S<sub>k</sub>**)
- **L'** = **h<sub>k</sub>WQ<sup>T</sup>**
- **A<sub>k</sub>** = Argmax(**L'**)

### Reward Definition
Initially, we apply the IRT model to learn the features of all test cases and all students' ground truth code qualities by maximizing **P(Y<sub>k, j</sub> \| θ<sub>j</sub>, d<sub>k</sub>)**, which are used to calculate the reward.

If the selected test case *k* has not been selected before, we use all the selected test cases to update student *j*'s current code quality estimate **θ̂<sub>j, k</sub>** by maximizing the **P(Y<sub>k, j</sub> \| θ̂<sub>j, k</sub>, d<sub>k</sub>)**. Then we calculate **1 / \|θ<sub>j</sub> - θ̂<sub>j, k</sub>\|** as the reward, which is **R<sub>k</sub>**. **R<sub>k</sub>** measures how close the student *j*'s current code quality estimate is to the corresponding ground truth code quality. The smaller the difference, the bigger the reward. Otherwise, **R<sub>k</sub>** = -10000. Giving a really big negative reward will force the agent to not select the same test case multiple times. We believe that the reward satisfies the Markovian property because it is solely based on the current state and action. Since the current state stores all the previously selected test cases.

### MDP definition
In summary, our MDP is defined as follows:

- S: LLM<sub>e</sub>(**q, a, t<sub>1..k</sub>**) - LLM<sub>e</sub>(**q, c<sub>j</sub>, t<sub>1..k</sub>**)
- A: the set of all test cases - {t<sub>1</sub>..t<sub>K</sub>}
- P: **P(LLM<sub>e</sub>(q, a, t<sub>1..k+1</sub>) \| LLM<sub>e</sub>(q, a, t<sub>1..k</sub>), A<sub>k</sub>)** = 1.0
- R:
    - -10000 if t<sub>k</sub> in {t<sub>1</sub>..t<sub>k-1</sub>}
    - **1 / \|θ<sub>j</sub> - θ̂<sub>j, k</sub>\|** otherwise
- **d<sub>0</sub>**: LLM<sub>e</sub>(**q, a**) - LLM<sub>e</sub>(**q, c<sub>j</sub>**)
- γ ∈ [0, 1), i.e., any value that is positive and smaller than 1.