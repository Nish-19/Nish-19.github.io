---
title: 'Unlabeled Data for Adversarial Robustness'
date: 2020-07-20
permalink: /posts/2020/07/blog-post-2/
tags:
  - Deep Learning
  - Adversarial Robustness
---

![image](https://user-images.githubusercontent.com/41947720/87966557-0a704580-cadb-11ea-965a-4b7d1046fc2f.png)
*Effect of adversarial perturbations on natural input - Misclassification of an image (which hasn't changed in human's perspective)*

This blog post talks about using Unlabeled data for improving Adversarial Robustness in deep neural networks. This post is a summary of the following works: -
1. [Using Pre-training can improve model robustness](https://arxiv.org/pdf/1901.09960.pdf) (ICML 2019)
2. [Unlabeled data improves adversarial robustness](https://arxiv.org/abs/1905.13736) (NeurIPS 2019)
3. [Are labels required for improving adversarial robustness?](https://arxiv.org/abs/1905.13725) (NeurIPS 2019)

## Brief Precursor to using unlabeled data for adversarial robustness

Adversarial Robustness, in general, deals with the question whether we can develop classifiers that are robust to (test time) perturbations of their input, by an adversary intending to fool the classifier. This is of prime importance in critical applications of deep learning like cancer recognition systems, self-driving cars etc. where the scope of error is almost nil. In the recent years there has been significant developments in creating both adversaries and defenses against them.

[Schmidt et al](https://papers.nips.cc/paper/7749-adversarially-robust-generalization-requires-more-data) had showed in their work that there is a sample complexity gap in achieving the same robust accuracy as clean accuracy for a classification task using cifar-10 as the dataset.

![image](https://user-images.githubusercontent.com/41947720/87968037-6c31af00-cadd-11ea-9a95-90e5f8da4ca0.png)

Hence from his work we can conclude that there is a need for additional data for improving adversarial robustness. This worked served as a motivation to the papers we will be discussing in this blog. The following researchers started wondering how to make up for this additional data to bridge the sample complexity gap.

## Using Pre-Training Improves Adversarial robustness

This paper introduces the concept of Adversarial Pre-training. This is based on the following concepts: -
1. The problem of requirement of more task specific data can be solved using pre-training (a typical transfer learning scenario)
2. Data from a different distribution can be beneficial for a different task (Huh et al)

### Method used

* Adversarial pre-training on downsampled (to 32X32 size) ImageNet(1000 class) dataset with 10 step PGD with eps = 8/255 (l-infinity).
* Fine tuning with Cifar-10 dataset using pgd-10 with eps = 8/255 for 5 epochs
* Finally, evaluating with cifar-10 using pgd-20 with eps = 8/255 (l-infinity)
Note - They use wrn-28-10 for all their experiments

### Results

![image](https://user-images.githubusercontent.com/41947720/87968988-eca4df80-cade-11ea-985c-1a585f6210dc.png)
From the results obtained by them we can observe that the clean accuracy has almost remained the same whereas the adversarial accuracy has significantly increased (by 12%). Hence from this work we can conclude that adversarial features can robustly transfer across data distributions.

## Unlabeled Data improves Adversarial robustness

This paper addresses the following questions -
* How can we account for additional data for improving robustness?
* How to get additional labelled data?
*Labelling may be an expensive process*

The solution to the questions is a Semi-Supervised Adversarial Training Algorithm

Here they propose an algorithm Robust Self Training (RST) which is based on : -
1. Taking unlabeled data and generating pseudo labels from them (using a network pre-trained with the labeled data)
2. Mixing the unlabeled data and the labeled data in a definite proportion.
3. Performing adversarial training ([TRADES](https://arxiv.org/pdf/1901.08573.pdf)) on this dataset.

The reasoning for using unlabeled data for bridging the sample complexity gap is: -
* Labeling Data is generally an expensive and tedious process.
* Adversarial Robustness requires the predictions to be stable around naturally occurring inputs. Achieving this doesnâ€™t really require labels.

### RST Algorithm pseudocode

![image](https://user-images.githubusercontent.com/41947720/87969963-aea8bb00-cae0-11ea-8c9a-40ac2c8b2b0d.png)

Here, Lstandard is the cross-entropy loss and Lrobust is the [KL](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) loss (as used in TRADES)

Note - Here cifar-10 is used as the labeled dataset containing 50K training samples. The unlabeled data is procured from the 80M Tiny Images dataset following a definite procedure. Total 500K unlabeled data is procured.

### Results

![image](https://user-images.githubusercontent.com/41947720/87970323-31ca1100-cae1-11ea-849a-3261f399a9b9.png)

*The results are reported using wrn-28-10 with learning_scheduler cosine and unsupervised_fraction - 0.5 (i.e. each epoch contains half labeled and half unlabeled data)*

We can observe that the RST model performs better than the other models on all the attacks, also it gets the highest accuracy on clean samples as well (89.7%)

## Are Labels Required for improving Adversarial Robustness

Like the previous work, this work also focuses on using unlabeled data for improving adversarial robustness.

The main contributions of this work are: -
1. Proposed UAT (Unsupervised Adversarial Training) method to make use of unlabeled data.
2. Proved that unlabeled data can be competitive to labelled data for bridging the sample complexity gap in Schmidt et al.
3. New state-of-art on CIFAR-10 using uncurated unlabeled data.

The motivation behind this work is: -
1. Labelled data is expensive
2. Adversarial robustness depends on the smoothness of the classifier which can be determined by unlabeled data.
3. Only a small amount of labelled data is needed for standard generalization.

Here, they propose three variants of the UAT algorithm and experiment with all three of them.

![image](https://user-images.githubusercontent.com/41947720/87971125-786c3b00-cae2-11ea-9c6f-5837d8f5c48e.png)

These loss notations are useful for understanding the pseudocodes below. As from the losses, both the losses are making use of the the adversarial perturbed input (by considering the l-infinity norm ball)

### Algorithm UAT-OT

![image](https://user-images.githubusercontent.com/41947720/87971483-0ba57080-cae3-11ea-9e30-8da4956e9009.png)

### Algorithm UAT-FT

![image](https://user-images.githubusercontent.com/41947720/87971531-1b24b980-cae3-11ea-9c16-4c0937b12aa1.png)

### Algorithm UAT++

![image](https://user-images.githubusercontent.com/41947720/87971577-2c6dc600-cae3-11ea-940e-c88c017d03fa.png)

Note the UAT++ algorithm is very similar to the RST algorithm of the previous paper except that in UAT++ both the losses (entropy and KL) are making use of the the adversarial perturbed input (by considering the l-infinity norm ball), whereas in RST only KL minimizes with adversarial input.

Also, here the unlabeled dataset is taken from the same labeled dataset but without considering the labels.

![image](https://user-images.githubusercontent.com/41947720/87971803-88384f00-cae3-11ea-8284-dadf0f41a486.png)

Here, we can see that the UAT++ algorithm performs as good as a supervised-oracle (a model which is trained with labels for the unlabeled data also). This proves that unlabeled data is competitive to labeled data for adversarial robustness.

## Conclusion

Hence we have seen how unlabeled data can be used to improve adversarial robustness by studying the above three papers. For more details regarding this it is recommended to read the original papers. For a general idea about adversarial robustness one can refer [here](https://adversarial-ml-tutorial.org/introduction/).

Stay tuned for more ML and DL content!
